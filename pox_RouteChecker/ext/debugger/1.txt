
from pox.core import core
import pox.openflow.libopenflow_01 as of
from pox.openflow.discovery import *

sender = LLDPSender()

class LLDP_Switch(EventMixin):
  def __init__(self, connection, ports):
    self.connection = connection
    self.ports = ports
    sender.addSwitch(connection.dpid, [(p.port_no, p.hw_addr) for p in ports])
    self.listenTo(connection)
  
  def _handle_PacketIn(self, event):
    print "PacketIn on %s:%s" % (event.connection.dpid, event.port)


class LLDP_Test(EventMixin):
  def __init__(self):
    self.listenTo(core.openflow)

  def _handle_ConnectionUp(self, event):
    sw = LLDP_Switch(event.connection, event.ofp.ports)

def launch():
  core.registerNew(LLDP_Test)
from pox.core import core
import pox.openflow.libopenflow_01 as of
from pox.openflow.discovery import *
from pox.lib.addresses import IPAddr


class TestController(EventMixin):

  IN = 1
  OUT = 2
  CHECK = 3
  LOOP = 4
  IDLE = 10
  HARD = 20

  def __init__(self):
    self.listenTo(core.openflow)
    self.listenTo(core.openflow_discovery)
    self.connections = {}
    self.hosts = {}
    self.restore_good_network()

  def _handle_LinkEvent(self, event):
    pass

  def restore_good_network(self):
    self.good_network = "10.0.0.0/24"

  def change_good_network(self):
    self.good_network = "10.0.0.0/23"

  def _handle_PacketIn(self, event):

    #select port on src that is connected to dst
    def select_port(src, dst):
      if src is None or dst is None:
        return None
      for l in core.openflow_discovery.adjacency:
        if l.dpid1 == src and l.dpid2 == dst:
          return l.port1
        elif l.dpid1 == dst and l.dpid2 == src:
          return l.port2
      return None

    #select port to output to
    def select_for_in(dpid, check):
      if dpid == self.IN:
        t = self.OUT
      elif dpid == self.OUT:
        t = self.IN
      else:
        t = None

      if check:
        return select_port(dpid, self.CHECK)
      else:
        return select_port(dpid, t)

    #return port from check to loop
    def check_port():
        return select_port(self.CHECK, self.LOOP)
     
    print "PacketIn on %s:%s" % (event.connection.dpid, event.port)
    packet = event.parse()

    #l2-learning connected hosts
    if packet.src not in self.hosts.keys():
      self.hosts[packet.src] = (event.connection.dpid, event.port)
      print "added ", packet.src, event.connection.dpid, event.port

    #all switches connected
    if len(self.connections.keys()) == 4:
      fm1 = of.ofp_flow_mod()
      fm1.match = of.ofp_match.from_packet(packet, event.port)
      p = None
      
      #flooding to neightbor hosts
      if packet.dst.isMulticast():
        print "flood"
        for host in self.hosts.keys():
          target_dpid, target_port = self.hosts[host]
          print target_dpid, target_port, event.port
          if event.connection.dpid == target_dpid and event.port != target_port:
            print "output ", target_port
            fm1.actions.append(of.ofp_action_output(port = target_port))
            fm1.buffer_id = event.ofp.buffer_id

      #connected to host      
      elif packet.dst in self.hosts:
        target_dpid, target_port = self.hosts[packet.dst]
        #we are directly connected to target host
        if event.connection.dpid == target_dpid and event.port != target_port:
          print "sending to ", event.connection.dpid, target_port
          fm1.actions.append(of.ofp_action_output(port = target_port))
          fm1.idle_timeout = self.IDLE
          fm1.hard_timeout = self.HARD
          fm1.buffer_id = event.ofp.buffer_id
          event.connection.send(fm1)
          return

      #creating path
      if event.connection.dpid == self.IN or event.connection.dpid == self.OUT:
        if event.connection.dpid == self.IN:
          other_side = self.OUT
        else:
          other_side = self.IN
        #from first hop to check|second
        check_needed = self.isCheckNeeded(packet)
        p = select_for_in(event.connection.dpid, check_needed)
        if p != event.port and p is not None:
          fm1.actions.append(of.ofp_action_output(port = p))
          fm1.idle_timeout = self.IDLE
          fm1.hard_timeout = self.HARD
          fm1.buffer_id = event.ofp.buffer_id
          event.connection.send(fm1)

          if check_needed:
            #from check to second
            fm1.match.in_port = check_port()
            fm1.actions = []
            p1 = select_port(self.CHECK, other_side)
            if p1 is not None:
              fm1.actions.append(of.ofp_action_output(port = p1))
              fm1.idle_timeout = self.IDLE
              fm1.hard_timeout = self.HARD
              fm1.buffer_id = -1

              core.openflow.sendToDPID(self.CHECK, fm1)
         
        elif len(fm1.actions) != 0:
          event.connection.send(fm1)
        #event.connection.send(po)
 
      elif event.connection.dpid == self.CHECK:
        #send all packets to loopback
        if event.port != check_port():
          fm2 = of.ofp_flow_mod()
          fm2.match = of.ofp_match.from_packet(packet)
          fm2.match.in_port = event.port
          fm2.actions.append(of.ofp_action_output(port = check_port()))
          fm2.idle_timeout = self.IDLE
          fm2.hard_timeout = self.HARD
          fm2.buffer_id = event.ofp.buffer_id
          event.connection.send(fm2)

      elif event.connection.dpid == self.LOOP:
        #send all packet back to input port
        fm3 = of.ofp_flow_mod()
        fm3.match = of.ofp_match.from_packet(packet, event.port)
        fm3.actions.append(of.ofp_action_output(port = of.OFPP_IN_PORT))
        fm3.idle_timeout = self.IDLE
        fm3.hard_timeout = self.HARD
        fm3.buffer_id = event.ofp.buffer_id
        event.connection.send(fm3)

  def _handle_ConnectionUp(self, event):
    id = event.connection.dpid
    if id == self.OUT:
      self.connections["out"] = event.connection
    elif id == self.CHECK:
      self.connections["check"] = event.connection
    elif id == self.LOOP:
      self.connections["loop"] = event.connection
    elif id == self.IN:
      self.connections["in"] = event.connection

  def isCheckNeeded(self, packet):
    ip = packet.find("ipv4")
    if ip is not None and (ip.srcip.inNetwork(self.good_network)):
      print ip.srcip, " not needed"
      return False
    else:
      return True

def launch():
  core.registerNew(TestController)

# Copyright 2011 James McCauley
# Copyright 2008 (C) Nicira, Inc.
#
# This file is part of POX.
#
# POX is free software: you can redistribute it and/or modify
# it under the terms of the GNU General Public License as published by
# the Free Software Foundation, either version 3 of the License, or
# (at your option) any later version.
#
# POX is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU General Public License for more details.
#
# You should have received a copy of the GNU General Public License
# along with POX.  If not, see <http://www.gnu.org/licenses/>.

# This file is based on the discovery component in NOX, though it has
# been substantially rewritten.

"""
This module discovers the connectivity between OpenFlow switches by sending
out LLDP packets. To be notified of this information, listen to LinkEvents
on core.Discovery.

It's possible that some of this should be abstracted out into a generic
Discovery module, or a Discovery superclass.
"""

from pox.lib.revent               import *
from pox.lib.recoco               import Timer
from pox.lib.packet.ethernet      import LLDP_MULTICAST, NDP_MULTICAST
from pox.lib.packet.ethernet      import ethernet
from pox.lib.packet.lldp          import lldp, chassis_id, port_id, end_tlv
from pox.lib.packet.lldp          import ttl, system_description
import pox.openflow.libopenflow_01 as of
from pox.lib.util                 import dpidToStr
from pox.core import core
from naive_logger import log

import struct
import array
import socket
import time
import copy
from collections import *

LLDP_TTL             = 120 # currently ignored
LLDP_SEND_CYCLE      = 5.0
TIMEOUT_CHECK_PERIOD = 5.0
LINK_TIMEOUT         = 10.0


class LLDPSender (object):
  """
  Cycles through a list of packets, sending them such that it completes the
  entire list every LLDP_SEND_CYCLE.
  """

  SendItem = namedtuple("LLDPSenderItem",
                      ('dpid','portNum','packet'))

  #NOTE: This class keeps the packets to send in a flat list, which makes
  #      adding/removing them on switch join/leave or (especially) port
  #      status changes relatively expensive. This could easily be improved.

  def __init__ (self):
    self._packets = []
    self._timer = None

  def addSwitch (self, dpid, ports):
    """ Ports are (portNum, portAddr) """
    self._packets = [p for p in self._packets if p.dpid != dpid]

    for portNum, portAddr in ports:
      if portNum > of.OFPP_MAX:
        # Ignore local
        continue
      self._packets.append(LLDPSender.SendItem(dpid, portNum,
       self.create_discovery_packet(dpid, portNum, portAddr)))

    self._setTimer()

  def delSwitch (self, dpid):
    self._packets = [p for p in self._packets if p.dpid != dpid]
    self._setTimer()

  def delPort (self, dpid, portNum):
    self._packets = [p for p in self._packets
                     if p.dpid != dpid or p.portNum != portNum]
    self._setTimer()

  def addPort (self, dpid, portNum, portAddr):
    if portNum > of.OFPP_MAX: return
    self.delPort(dpid, portNum)
    self._packets.append(LLDPSender.SendItem(dpid, portNum,
     self.create_discovery_packet(dpid, portNum, portAddr)))
    self._setTimer()

  def _setTimer (self):
    if self._timer: self._timer.cancel()
    self._timer = None
    if len(self._packets) != 0:
      self._timer = Timer(LLDP_SEND_CYCLE / len(self._packets),
                          self._timerHandler, recurring=True)

  def _timerHandler (self):
    """
    Called by a timer to actually send packet.
    Picks the first packet off the queue, sends it, and puts it back on the
    end of the queue.
    """
    item = self._packets.pop(0)
    self._packets.append(item)
    #NOTE: Changed from original core.openflow
    core.proxy.sendToDPID(item.dpid, item.packet, True)

  def create_discovery_packet (self, dpid, portNum, portAddr):
    """ Create LLDP packet """

    discovery_packet = lldp()
    
    cid = chassis_id()
    # Maybe this should be a MAC.  But a MAC of what?  Local port, maybe?
    cid.fill(cid.SUB_LOCAL, bytes('dpid:' + hex(long(dpid))[2:-1]))
    discovery_packet.add_tlv(cid)

    pid = port_id()
    pid.fill(pid.SUB_PORT, str(portNum))
    discovery_packet.add_tlv(pid)

    ttlv = ttl()
    ttlv.fill(LLDP_TTL)
    discovery_packet.add_tlv(ttlv)

    sysdesc = system_description()
    sysdesc.fill(bytes('dpid:' + hex(long(dpid))[2:-1]))
    discovery_packet.add_tlv(sysdesc)

    discovery_packet.add_tlv(end_tlv())
    
    eth = ethernet()
    eth.src = portAddr
    eth.dst = NDP_MULTICAST
    eth.set_payload(discovery_packet)
    eth.type = ethernet.LLDP_TYPE
    po = of.ofp_packet_out(action = of.ofp_action_output(port=of.OFPP_IN_PORT),
                           in_port=portNum, data = eth.pack())
    return po.pack()


class LinkEvent (Event):
  def __init__ (self, add, link):
    Event.__init__(self)
    self.link = link
    self.added = add
    self.removed = not add

  def portForDPID (self, dpid):
    if self.link.dpid1 == dpid:
      return self.link.port1
    if self.link.dpid2 == dpid:
      return self.link.port2
    return None


class Discovery (EventMixin):
  """
  Component that attempts to discover topology.
  Works by sending out LLDP packets
  discovery application for topology inference
  """

  _eventMixin_events = set([
    LinkEvent,
  ])

  _core_name = "proxy_discovery" # we want to be core.proxy_discovery

  Link = namedtuple("Link",("dpid1","port1","dpid2","port2"))

  def __init__ (self, install_flow = True, explicit_drop = True, 
                init_sender = False):
    self.explicit_drop = explicit_drop
    self.install_flow = install_flow
    self.init_sender = init_sender

    self._dps = set()
    self.adjacency = {} # From Link to time.time() stamp
    if self.init_sender:
      self._sender = LLDPSender()
    Timer(TIMEOUT_CHECK_PERIOD, self._expireLinks, recurring=True)

    if core.hasComponent("proxy"):
      self.listenTo(core.proxy)
    else:
      # We'll wait for PROXY (not openflow) to come up
      self.listenTo(core)
    core.register("proxy_discovery", self)

  def _handle_ComponentRegistered (self, event):
    if event.name == "proxy":
      self.listenTo(core.proxy)
      return EventRemove # We don't need this listener anymore

  def _handle_ConnectionUp (self, event):
    """ On datapath join, create a new LLDP packet per port """
    assert event.dpid not in self._dps

    """
    We dont want to show our presence so installing 
    flow must be commented for now
    if self.install_flow:
      log.debug("Installing flow for %s", dpidToStr(event.dpid))
      msg = of.ofp_flow_mod(match = of.ofp_match(dl_type = ethernet.LLDP_TYPE,
                                                 dl_dst = NDP_MULTICAST))
      msg.actions.append(of.ofp_action_output(port = of.OFPP_CONTROLLER))
      event.connection.send(msg)
    """
    
    self._dps.add(event.dpid)
    log.info ("ConnectionUp " + str(event.dpid))
    if self.init_sender:
      self._sender.addSwitch(event.dpid, [(p.port_no, p.hw_addr)
                                        for p in event.ofp.ports])

  def _handle_ConnectionDown (self, event):
    """ On datapath leave, delete all associated links """
    assert event.dpid in self._dps

    self._dps.remove(event.dpid)
    if self.init_sender:
      self._sender.delSwitch(event.dpid)

    deleteme = []
    for link in self.adjacency:
      if link.dpid1 == event.dpid or link.dpid2 == event.dpid:
        deleteme.append(link)

    self._deleteLinks(deleteme)

  def _handle_PortStatus (self, event):
    '''
    Update the list of LLDP packets if ports are added/removed

    Add to the list of LLDP packets if a port is added.
    Delete from the list of LLDP packets if a port is removed.
    '''
    # Only process 'sane' ports
    if event.port <= of.OFPP_MAX and self.init_sender:
      if event.added:
        self._sender.addPort(event.dpid, event.port, event.ofp.desc.hw_addr)
      elif event.deleted:
        self._sender.delPort(event.dpid, event.port)

  def _expireLinks (self):
    '''
    Called periodially by a timer to expire links that haven't been
    refreshed recently.
    '''
    curtime = time.time()

    deleteme = []
    for link,timestamp in self.adjacency.iteritems():
      if curtime - timestamp > LINK_TIMEOUT:
        deleteme.append(link)
        log.info('link timeout: %s.%i -> %s.%i' %
                 (dpidToStr(link.dpid1), link.port1,
                  dpidToStr(link.dpid2), link.port2))

    if deleteme:
      self._deleteLinks(deleteme)

  def _handle_PacketIn (self, event):
    """ Handle incoming lldp packets.  Use to maintain link state """
    packet = event.parsed

    if packet.type != ethernet.LLDP_TYPE: return
    if packet.dst != NDP_MULTICAST: return

    if not packet.next:
      log.error("lldp packet could not be parsed")
      return

    assert isinstance(packet.next, lldp)

    if self.explicit_drop:
      if event.ofp.buffer_id != -1:
        log.debug("Dropping LLDP packet %i", event.ofp.buffer_id)
        msg = of.ofp_packet_out()
        msg.buffer_id = event.ofp.buffer_id
        msg.in_port = event.port
        #Switch!
        event.connection.switch.send(msg)

    lldph = packet.next
    if  len(lldph.tlvs) < 3 or \
      (lldph.tlvs[0].tlv_type != lldp.CHASSIS_ID_TLV) or\
      (lldph.tlvs[1].tlv_type != lldp.PORT_ID_TLV) or\
      (lldph.tlvs[2].tlv_type != lldp.TTL_TLV):
      log.error("lldp_input_handler invalid lldp packet")
      return

    def lookInSysDesc():
      r = None
      for t in lldph.tlvs[3:]:
        if t.tlv_type == lldp.SYSTEM_DESC_TLV:
          # This is our favored way...
          for line in t.next.split('\n'):
            if line.startswith('dpid:'):
              try:
                return int(line[5:], 16)
              except:
                pass
          if len(t.next) == 8:
            # Maybe it's a FlowVisor LLDP...
            try:
              return struct.unpack("!Q", t.next)[0]
            except:
              pass
          return None

    originatorDPID = lookInSysDesc()

    if originatorDPID == None:
      # We'll look in the CHASSIS ID
      if lldph.tlvs[0].subtype == chassis_id.SUB_LOCAL:
        if lldph.tlvs[0].id.startswith('dpid:'):
          # This is how NOX does it at the time of writing
          try:
            originatorDPID = int(lldph.tlvs[0].id.tostring()[5:], 16)
          except:
            pass
      if originatorDPID == None:
        if lldph.tlvs[0].subtype == chassis_id.SUB_MAC:
          # Last ditch effort -- we'll hope the DPID was small enough
          # to fit into an ethernet address
          if len(lldph.tlvs[0].id) == 6:
            try:
              s = lldph.tlvs[0].id
              originatorDPID = struct.unpack("!Q",'\x00\x00' + s)[0]
            except:
              pass


    if originatorDPID == None:
      log.warning("Couldn't find a DPID in the LLDP packet")
      return

    # if chassid is from a switch we're not connected to, ignore
    if originatorDPID not in self._dps:
      log.info('Received LLDP packet from unconnected switch')
      return
    
    # grab port ID from port tlv
    if lldph.tlvs[1].subtype != port_id.SUB_PORT:
      log.warning("Thought we found a DPID, but packet didn't have a port")
      return # not one of ours
    originatorPort = None
    if lldph.tlvs[1].id.isdigit():
      # We expect it to be a decimal value
      originatorPort = int(lldph.tlvs[1].id)
    elif len(lldph.tlvs[1].id) == 2:
      # Maybe it's a 16 bit port number...
      try:
        originatorPort  =  struct.unpack("!H", lldph.tlvs[1].id)[0]
      except:
        pass
    if originatorPort is None:
      log.warning("Thought we found a DPID, but port number didn't " +
                  "make sense")
      return

    if (event.dpid, event.port) == (originatorDPID, originatorPort):
      log.error('Loop detected; received our own LLDP event')
      return

    # print 'LLDP packet in from',chassid,' port',str(portid)

    link = Discovery.Link(originatorDPID, originatorPort, event.dpid,
                          event.port)

    if link not in self.adjacency:
      self.adjacency[link] = time.time()
      log.info('link detected: %s.%i -> %s.%i' %
               (dpidToStr(link.dpid1), link.port1,
                dpidToStr(link.dpid2), link.port2))
      self.raiseEventNoErrors(LinkEvent, True, link)
    else:
      # Just update timestamp
      self.adjacency[link] = time.time()
    return EventHalt # Probably nobody else needs this event

  def _deleteLinks (self, links):
    for link in links:
      del self.adjacency[link]
      self.raiseEvent(LinkEvent, False, link)


  def isSwitchOnlyPort (self, dpid, port):
    """ Returns True if (dpid, port) designates a port that has any
    neighbor switches"""
    for link in self.adjacency:
      if link.dpid1 == dpid and link.port1 == port:
        return True
      if link.dpid2 == dpid and link.port2 == port:
        return True
    return False

  def print_adjacency(self):
    for link in self.adjacency:
      print "%s:%s ---- %s:%s" % (str(link.dpid1), str(link.port1), 
                                  str(link.dpid2), str(link.port2))
      
  def get_other_end(self, dpid, port):
    for link in self.adjacency:
      if link.dpid1 == dpid and link.port1 == port:
        return (link.dpid2, link.port2)
      elif link.dpid2 == dpid and link.port2 == port:
        return (link.dpid1, link.port1)
    return None


def launch (explicit_drop = False, install_flow = True):
  explicit_drop = str(explicit_drop).lower() == "true"
  install_flow = str(install_flow).lower() == "true"
  core.registerNew(Discovery, explicit_drop=explicit_drop,
                   install_flow=install_flow)
# Copyright 2011 James McCauley
#
# This file is part of POX.
#
# POX is free software: you can redistribute it and/or modify
# it under the terms of the GNU General Public License as published by
# the Free Software Foundation, either version 3 of the License, or
# (at your option) any later version.
#
# POX is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU General Public License for more details.
#
# You should have received a copy of the GNU General Public License
# along with POX.  If not, see <http://www.gnu.org/licenses/>.

"""
This is the main OpenFlow module.
 .connection - a reference to the switch connection that caused the event
    CHANGED: reference to ConnectionPair from debug_proxy
 .dpid - the DPID of the switch that caused the event
 .ofp - the OpenFlow message that caused the event (from libopenflow)

One of the more complicated aspects of OpenFlow is dealing with stats
replies, which may come in multiple parts (it shouldn't be that that
difficult, really, but that hasn't stopped it from beind handled wrong
wrong more than once).  In POX, the raw events are available, but you will
generally just want to listen to the aggregate stats events which take
care of this for you and are only fired when all data is available.
"""

from pox.lib.revent import *
import pox.openflow.libopenflow_01 as of
from pox.lib.packet.ethernet import ethernet

class ResendController:
  """
  Message has an instance of this class inside.
  It hepls to collect information about whether
  this message needs to be resent or not.
  """
  def __init__(self):
    self._resend = None
  
  def resend(self):
    if self._resend is None:
      self._resend = True
    else:
      self._resend |= True

  def no_resend(self):
    if self._resend is None:
      self._resend = False
    else:
      self._resend |= False

  def resend_needed(self):
    if self._resend is None:
      return True
    else:
      return self._resend
  
  def __str__(self):
    return "Resend: " + str(self._resend)


class OFEvent (Event):
  """
  Base class for our proxy openflow events.
  Connection is instance of ConnectionPair
  resend is instance of ResendController
  """
  def __init__(self, connection, ofp, resend = None):
    Event.__init__(self)
    self.connection = connection
    self.dpid = connection.dpid
    self.ofp = ofp
    self.resend = resend


  def __str__(self):
    return str(self.connection) +'\n' + str(self.dpid) + '\n' \
            + str(self.ofp) + '\n' + str(self.resend)
  
class ConnectionUp (OFEvent):
  """
  Connection raised when the connection to an OpenFlow switch has been
  established.
  """
  def __init__ (self, connection, ofp, resend = None):
    OFEvent.__init__(self, connection, ofp, resend)

class ConnectionDown (Event):
  """
  Connection raised when the connection to an OpenFlow switch has been
  lost.
  """
  def __init__ (self, connection):
    self.connection = connection

class PortStatus (OFEvent):
  """
  Fired in response to port status changes.
  added (bool) - True if fired because a port was added
  deleted (bool) - True if fired because a port was deleted
  modified (bool) - True if fired because a port was modified
  port (int) - number of port in question
  """
  def __init__ (self, connection, ofp, resend = None):
    OFEvent.__init__(self, connection, ofp, resend)
    self.modified = ofp.reason == of.OFPPR_MODIFY
    self.added = ofp.reason == of.OFPPR_ADD
    self.deleted = ofp.reason == of.OFPPR_DELETE
    self.port = ofp.desc.port_no

class FlowRemoved (OFEvent):
  """
  Raised when a flow entry has been removed from a flow table.
  This may either be because of a timeout or because it was removed
  explicitly.
  Properties:
  idleTimeout (bool) - True if expired because of idleness
  hardTimeout (bool) - True if expired because of hard timeout
  timeout (bool) - True if either of the above is true
  deleted (bool) - True if deleted explicitly
  """
  def __init__ (self, connection, ofp, resend = None):
    OFEvent.__init__(self, connection, ofp, resend)
    self.idleTimeout = False
    self.hardTimeout = False
    self.deleted = False
    self.timeout = False
    if ofp.reason == of.OFPRR_IDLE_TIMEOUT:
      self.timeout = True
      self.idleTimeout = True
    elif ofp.reason == of.OFPRR_HARD_TIMEOUT:
      self.timeout = True
      self.hardTimeout = True
    elif ofp.reason == of.OFPRR_DELETE:
      self.deleted = True

class RawStatsReply (OFEvent):
  def __init__ (self, connection, ofp, resend = None):
    OFEvent.__init__(self, connection, ofp, resend)
    # Raw ofp message(s)

def _processStatsBody(body, obj):
  r = []
  t = obj.__class__
  remaining = len(body)
  while remaining:
    obj = t()
    body = obj.unpack(body)
    assert len(body) < remaining
    remaining = len(body)
    r.append(obj)
  return r


class StatsReply (OFEvent):
  """ Abstract superclass for all stats replies """
  def __init__ (self, connection, parts):
    OFEvent.__init__(self, connection, parts, None)
    #Processed
    self.stats = None

class SwitchDescReceived (StatsReply):
  def __init__(self, connection, parts):
    StatsReply.__init__(self, connection, parts)
    self.stats = of.ofp_desc_stats()
    self.stats.unpack(parts[0].body)

class FlowStatsReceived (StatsReply):
  def __init__(self, connection, parts):
    StatsReply.__init__(self, connection, parts)
    self.stats = []
    for part in parts:
      self.stats += _processStatsBody(part.body, of.ofp_flow_stats())

class AggregateFlowStatsReceived (StatsReply):
  def __init__(self, connection, parts):
    StatsReply.__init__(self, connection, parts)
    self.stats = of.ofp_aggregate_stats_reply()
    self.stats.unpack(parts[0].body)

class TableStatsReceived (StatsReply):
  def __init__(self, connection, parts):
    StatsReply.__init__(self, connection, parts)
    self.stats = []
    for part in parts:
      self.stats += _processStatsBody(part.body, of.ofp_table_stats())

class PortStatsReceived (StatsReply):
  def __init__(self, connection, parts):
    StatsReply.__init__(self, connection, parts)
    self.stats = []
    for part in parts:
      self.stats += _processStatsBody(part.body, of.ofp_port_stats())

class QueueStatsReceived (StatsReply):
  def __init__(self, connection, parts):
    StatsReply.__init__(self, connection, parts)
    self.stats = []
    for part in parts:
      self.stats += _processStatsBody(part.body, of.ofp_queue_stats())

class PacketIn (OFEvent):
  """
  Fired in response to PacketIn events
  port (int) - number of port the packet came in on
  data (bytes) - raw packet data
  parsed (packet subclasses) - pox.lib.packet's parsed version
  """
  def __init__ (self, connection, ofp, resend = None):
    OFEvent.__init__(self, connection, ofp, resend)
    self.port = ofp.in_port
    self.data = ofp.data
    self._parsed = None

  def parse (self):
    if self._parsed is None:
      self._parsed = ethernet(self.data)
    return self._parsed

  @property
  def parsed (self):
    """
    The packet as parsed by pox.lib.packet
    """
    return self.parse()

class ErrorIn (OFEvent):
  def __init__ (self, connection, ofp, resend = None):
    OFEvent.__init__(self, connection, ofp, resend)
    self.xid = ofp.xid

  def asString (self):
    return self.ofp.show()

class BarrierIn (OFEvent):
  """
  Fired in response to a barrier reply
  xid (int) - XID of barrier request
  """
  def __init__ (self, connection, ofp, resend = None):
    OFEvent.__init__(self, connection, ofp, resend)
    self.xid = ofp.xid

class PacketOut (OFEvent):
  
  def __init__ (self, connection, ofp, resend = None):
    OFEvent.__init__(self, connection, ofp, resend)

class FlowMod (OFEvent):
  def __init__ (self, connection, ofp, resend = None):
    OFEvent.__init__(self, connection, ofp, resend)

class PortMod (OFEvent):
  def __init__ (self, connection, ofp, resend = None):
    OFEvent.__init__(self, connection, ofp, resend)

class StatsRequest (OFEvent):
  def __init__ (self, connection, ofp, resend = None):
    OFEvent.__init__(self, connection, ofp, resend)

class FeaturesRequest (OFEvent):
  def __init__ (self, connection, ofp, resend = None):
    OFEvent.__init__(self, connection, ofp, resend)

class FeaturesReply (OFEvent):
  def __init__ (self, connection, ofp, resend = None):
    OFEvent.__init__(self, connection, ofp, resend)

class Hello (OFEvent):
  def __init__ (self, connection, ofp, resend = None):
    OFEvent.__init__(self, connection, ofp, resend)

class EchoRequest (OFEvent):
  def __init__ (self, connection, ofp, resend = None):
    OFEvent.__init__(self, connection, ofp, resend)

class EchoReply (OFEvent):
  def __init__ (self, connection, ofp, resend = None):
    OFEvent.__init__(self, connection, ofp, resend)

class BarrierRequest (OFEvent):
  def __init__ (self, connection, ofp, resend = None):
    OFEvent.__init__(self, connection, ofp, resend)

class SwitchConfig (OFEvent):
  def __init__ (self, connection, ofp, resend = None):
    OFEvent.__init__(self, connection, ofp, resend)
    
def ofp_to_OFEvent(msg):
  if isinstance(msg, of.ofp_hello):
    return Hello
  elif isinstance(msg, of.ofp_port_status):
    return PortStatus
  elif isinstance(msg, of.ofp_flow_mod):
    return FlowMod
  elif isinstance(msg, of.ofp_port_mod):
    return PortMod
  elif isinstance(msg, of.ofp_packet_in):
    return PacketIn
  elif isinstance(msg, of.ofp_flow_removed):
    return FlowRemoved
  elif isinstance(msg, of.ofp_features_request):
    return FeaturesRequest
  elif isinstance(msg, of.ofp_features_reply):
    return FeaturesReply
  elif isinstance(msg, of.ofp_error):
    return ErrorIn
  elif isinstance(msg, of.ofp_packet_out):
    return PacketOut
  elif isinstance(msg, of.ofp_echo_request):
    return EchoRequest
  elif isinstance(msg, of.ofp_echo_reply):
    return EchoReply
  elif isinstance(msg, of.ofp_barrier_request):
    return BarrierRequest
  elif isinstance(msg, of.ofp_barrier_reply):
    return BarrierIn
  elif isinstance(msg, of.ofp_stats_request):
    return StatsRequest
  elif isinstance(msg, of.ofp_stats_reply):
    return RawStatsReply
  elif isinstance(msg, of.ofp_switch_config):
    return SwitchConfig
  else:
    return None

def ofp_to_OFStats(parts):
  if parts[0].type == of.OFPST_DESC:
    return SwitchDescReceived    
  elif parts[0].type == of.OFPST_FLOW:
    return FlowStatsReceived
  elif parts[0].type == of.OFPST_AGGREGATE:
    return AggregateFlowStatsReceived
  elif parts[0].type == of.OFPST_TABLE:
    return TableStatsReceived
  elif parts[0].type == of.OFPST_PORT:
    return PortStatsReceived
  elif parts[0].type == of.OFPST_QUEUE:
    return QueueStatsReceived
  else:
    return None
import socket
import select
import threading
import os
import sys
import exceptions
from errno import EAGAIN, ECONNRESET

from pox.lib.recoco.recoco import Task, Select
import pox.openflow.libopenflow_01 as of
from database import Database
import pox.lib.packet as pkt
from pox.core import core
from pox.lib.revent import EventMixin
from openflow_events import *
from connections import MessageIn, BaseConnection, \
    SwitchConnection, ControllerConnection
from naive_logger import log
from discovery import *

listener = None

class ConnectionPair():
  def __init__(self, s, c, dpid = None):
    self.switch = s
    self.controller = c
    self.dpid = dpid

  def send(self, data):
    return self.switch.send(data)

db = None
proxy = None
discovery = None

class Proxy(EventMixin):

  _eventMixin_events = set([
    ConnectionUp,
    ConnectionDown,
    PortStatus,
    FlowRemoved,
    PacketIn,
    ErrorIn,
    BarrierIn,
    RawStatsReply,
    SwitchDescReceived,
    FlowStatsReceived,
    AggregateFlowStatsReceived,
    TableStatsReceived,
    PortStatsReceived,
    QueueStatsReceived,

    PacketOut, 
    Hello, 
    FlowMod,
    PortMod,
    FeaturesRequest,
    FeaturesReply,
    EchoRequest,
    EchoReply,
    BarrierRequest,
    StatsRequest,
    SwitchConfig
  ])

  def __init__(self, target_port = 6634, target_address = "0.0.0.0"):
    self.target_port = target_port
    self.target_address = target_address
    self.connections = {}
    self.max_index = 0
    self.matches = {}
    self.waiting_packets_checked = time.gmtime()
    self.waiting_packets = {}
    core.register('proxy', self)
    self.listenTo(core.proxy)

  def create_pair(self, new_sock):
    self.max_index += 1
    newcon = SwitchConnection(new_sock, self.max_index)
    log.debug("switch connection" + str(newcon))
    try:
      control = ControllerConnection(self.target_port, self.target_address, self.max_index)
      log.debug("controller connection" + str(control))
      self.listenTo(newcon)
      self.listenTo(control)
      pair = ConnectionPair(newcon, control)
      self.connections[self.max_index] = pair
      return pair
    except Exception as e:
      log.debug(str(e))
      return None
    
  def remove(self, connection):
    if isinstance(connection, BaseConnection):
      return self.connections.pop(connection.ID, None)

  def _handle_MessageIn(self, event):
    self.process_message(event.connection, event.ofp)

  def _handle_StatsIn(self, event):
    self.process_stats(event.connection, event.ofp)

  def process_message(self, connection, msg):
    """
    Send an event to listeners
    NOTE: We are dependent on not-copying message during processing
          We have single instance of ResendController for all listeners
          Also we will resend the result message after pipeline of listeners.
    """
    if isinstance(connection, BaseConnection):
      event = ofp_to_OFEvent(msg)
      if event is not None:
        #Collect full stats reply
        if event == RawStatsReply:
          connection._incoming_stats_reply(msg)      
        resend = ResendController()
        self.raiseEventNoErrors(event, self.connections[connection.ID], msg, resend)
        if resend.resend_needed():
          self.resend_message(connection, msg)
                
  def process_stats(self, connection, parts):
    if isinstance(connection, BaseConnection):
      stats = ofp_to_OFStats(parts)
      if stats is not None:
        self.raiseEventNoErrors(stats, self.connections[connection.ID], parts)

  def resend_message(self, connection, msg):
      message = msg.__class__.__name__
      result = self.connections[connection.ID]
      if result.switch is connection:
        result.controller.send(msg)
        log.info("%-20s \t %s \t->\t %s" % (message, connection, result.controller))
      elif result.controller is connection:
        result.switch.send(msg)
        log.info("%-20s \t %s \t->\t %s" % (message, connection, result.switch)) 

  def sendToDPID(self, dpid, packet, switch = True):
    for conID, pair in self.connections.iteritems():
      if pair.dpid == dpid:
        try:
          if switch == True:
            pair.switch.send(packet)
          else:
            pair.controller.send(packet)
        except:
          pass
        break

  def send_message(self, connection, msg):
    connection.send(msg)
    
  def _handle_ConnectionDown(self, event):
    self.raiseEventNoErrors(event)

  def _handle_FlowMod(self, event):
    #We are going to resend this PacketIn
    self.check_waiting_packets(event.dpid, event.ofp.buffer_id)
    #To prevent duplicated actions
    #TODO: smth with overlapping FlowMods and duplicated matches
    if len([action for action in event.ofp.actions if \
      isinstance(action, of.ofp_action_output) and \
      action.port == of.OFPP_CONTROLLER]) == 0:
      log.info("Adding debug-force")
      event.ofp.actions.insert(0, of.ofp_action_output(\
              port = of.OFPP_CONTROLLER))
    else:
      dpid = event.dpid
      if event.ofp.match not in self.matches[dpid]:
        self.matches[dpid].append(event.ofp.match)
        log.info("matches for dpid %s : %s" % (dpid, len(self.matches[dpid])))
    event.ofp.flags |= of.OFPFF_SEND_FLOW_REM
    log.debug("FlowMod: %s" % (event.ofp))
    event.resend.resend()


  def store_and_wait(self, ofp, dpid, in_port, src_dpid, src_port, buffer_id):
    self.waiting_packets[(dpid, buffer_id)] = (time.time(), ofp, dpid, in_port, src_dpid, src_port)

  def check_waiting_packets(self, dpid = None, buffer_id = None):
    #This PacketIn is going to be resend.
    #Prevent from duplication
    if buffer_id != -1 and buffer_id is not None \
            and (dpid, buffer_id) in self.waiting_packets:
      print "Deleted info about %s:%s" % (str(dpid), str(self.waiting_packets[dpid, buffer_id][3]))
      del self.waiting_packets[(dpid, buffer_id)]

    #Delete unused packets
    #And store PacketIn in Database
    cur_time = time.time()
    if cur_time > self.waiting_packets_checked:
      for key, val in self.waiting_packets.items():
        if cur_time - val[0] > 2:
          data = val
          db.add_history(packet=pkt.ethernet(data[1].data), \
                     dpid=data[2], in_port=data[3], \
                     src_dpid=data[4], src_port=data[5], t=time.gmtime(data[0]))
          log.debug("PacketIN on %s:%s : %s Data: %s" % \
                (data[2], data[3],\
                 data[1] , pkt.ethernet(data[1].data)))

          del self.waiting_packets[key]

    self.waiting_packets_checked = cur_time

  def _handle_PacketIn(self, event):
    try:
      src = core.proxy_discovery.get_other_end(event.dpid, event.ofp.in_port)
      if src is None:
        dpid, port = None, None
      else:
        dpid, port = src
    except Exception as e:
      log.error(str(e))
      
    new_match = of.ofp_match.from_packet(\
                pkt.ethernet(event.ofp.data), event.ofp.in_port)
    if event.ofp.reason == of.OFPR_NO_MATCH or \
        len([match for match in self.matches[event.dpid] \
        if match.matches_with_wildcards(new_match)]) > 0:
      if event.ofp.reason == of.OFPR_NO_MATCH: 
        log.debug("NO MATCH")
      #We will store info if there will be no response
      self.store_and_wait(event.ofp, \
                     event.dpid, event.ofp.in_port,\
                     dpid, port, event.ofp.buffer_id)
      event.resend.resend()
    else:
      db.add_history(packet=pkt.ethernet(event.ofp.data), \
                     dpid=event.dpid, in_port=event.ofp.in_port, \
                     src_dpid=dpid, src_port=port, t=time.gmtime())

      event.resend.no_resend()
      log.info("Didn't send to controller")

  def _handle_PacketOut(self, event):
    log.debug("PacketOut on %s : %s" % \
        (event.dpid, event.ofp))
    log.debug("Data: %s" % (pkt.ethernet(event.ofp.data)))
    for action in event.ofp.actions:
      if action.port == OFPP_CONTROLLER:
        self.check_waiting_packets(event.dpid, event.ofp.buffer_id)
        break
    event.resend.resend()

  def _handle_FeaturesReply(self, event):
    #Learning real DPIDs
    self.connections[event.connection.switch.ID].dpid = event.ofp.datapath_id
    self.matches[event.ofp.datapath_id] = []
    self.raiseEventNoErrors(ConnectionUp, event.connection, \
            event.ofp, event.resend)
    event.resend.resend()

  def _handle_FlowRemoved(self, event):
    dpid = event.dpid
    try:
      self.matches[dpid].remove(event.ofp.match)
    except:
      log.debug("No such matches")
    log.debug("FlowRemoved: %s" % (event.ofp))
    log.info("matches for dpid %s : %s" % (dpid, len(self.matches[dpid])))
    event.resend.resend()

class ProxyTask(Task):
  def __init__(self, port = 6633, address = '0.0.0.0', 
               target_port = 6634, target_address = "0.0.0.0"):
    global proxy, db, discovery
    
    db = Database()
    db.clear()
    proxy = Proxy(target_port, target_address)
    discovery = Discovery()
    Task.__init__(self)
    self.port = int(port)
    self.address = address
    self.target_port = int(target_port)
    self.target_address = target_address
    core.register(self, 'ProxyTask')
    self.start()

  def run(self):
    self.sockets = []
    listener = socket.socket(socket.AF_INET, socket.SOCK_STREAM)
    listener.setsockopt(socket.SOL_SOCKET, socket.SO_REUSEADDR, 1)
    listener.bind((self.address, self.port))
    listener.listen(10000)
    self.sockets.append(listener)
    print("Listening on %s:%s" % (self.address, self.port))
    
    con = None
    while (True):
      try:
        while True:
          con = None
          rlist, wlist, elist = yield Select(self.sockets, [], self.sockets, 5)
          #log.debug("read " + str(len(rlist)) + ", ex " + str(len(elist)))
          if len(rlist) == 0 and len(wlist) == 0 and len(elist) == 0:
            """
            try:
              timer_callback()
            except:
              print "[Timer]", sys.exc_info
            continue
            """
            #if not core.running: break
            pass
          
          for con in elist:
            if con is listener:
              raise RuntimeError("Error on listener socket")
            else:
              try:
                self.remove_connection(con)
              except Exception as e:
                log.error(str(e))
              
          for con in rlist:
            if con is listener:
              new_sock = listener.accept()[0]
              print("Connection accepted")
              
              #if pox.openflow.debug.pcap_traces:
                #new_sock = wrap_socket(new_sock)
              new_sock.setblocking(0)
              try:
                self.create_pair(new_sock)
              except Exception as e:
                log.error(str(e))
              
            else:
              #We will work as a timer!
              core.proxy.check_waiting_packets()
              if not con.read():
                try:
                  log.debug("Bad reading from " + str(con))
                  self.remove_connection(con)
                except Exception as e:
                  log.error(str(e))
                
      except exceptions.KeyboardInterrupt:
        break
      except Exception as e:
        log.error(str(e))
        doTraceback = True
        if sys.exc_info()[0] is socket.error:
          if sys.exc_info()[1][0] == ECONNRESET:
            con.info("Connection reset")
            doTraceback = False

        if doTraceback:
          log.debug("Exception reading connection " + str(con))

        if con is listener:
          log.error("Exception on OpenFlow listener.  Aborting.")
          break
        try:
          self.remove_connection(con)
        except Exception as e:
          log.error(str(e))
      
  def create_pair(self, socket):
    #log.debug("create_pair")
    result = proxy.create_pair(socket)
    if result is not None:
      self.sockets.append( result.switch )
      self.sockets.append( result.controller )
      log.debug(str(result.switch) + " connected with " + str(result.controller))
      log.debug("Sockets: " + str(len(self.sockets)))

  def remove_connection(self, con):
    result = proxy.remove(con)
    if result is not None:
      try:
        result.switch.close()
      except:
        pass
      try:
        result.controller.close()
      except:
        pass
      self.sockets.remove(result.switch)
      self.sockets.remove(result.controller)

class Log():
  def __init__(self):
    self.debug_file = file("debug.log", "w")
    self.info_file = file("info.log", "w")

  def debug(self, string):
    #print "DEBUG: " + string
    self.debug_file.write("DEBUG: " + string + "\n")
    #self.f.flush()
    pass
    
  def error(self, string):
    print "ERROR: " + string
    
  def info(self, string):
    #print "INFO: " + string
    self.info_file.write("INFO: " + string + "\n")
    #self.f.flush()
    pass

  def flush(self):
    self.debug_file.flush()
    self.info_file.flush()

  def raw(self, string):
    self.debug_file.write(string)
    
log = Log()

import exceptions
import collections
from database import Database
import pox.openflow.libopenflow_01 as of
from pox.lib.addresses import *
import time
import networkx as nx
import pylab
from naive_logger import log

#What shall DbChecker return from SELECT
NOTHING = 0
COUNT = 1
FULL = 2
RETURN = 3

class FlowMatch:
    """
    Used to store flow information
    Can store multiple values of each parameter
    Also can store ip network addresses: address/mask
    """
    def __init__(self, params):
        """
        Params must be a dictionary {string: <tuple/list of strings>} 
        or {string: string}
        Analyzed parameters' keys are in self.headers
        """
        self.headers = [('dl_src', EthAddr), 
                        ('dl_dst', EthAddr), 
                        ('dl_vlan', int), 
                        ('dl_vlan_pcp', int), 
                        ('dl_type', int),
                        ('nw_tos', int), 
                        ('nw_proto', int), 
                        ('nw_src', IPAddr), 
                        ('nw_dst', IPAddr),
                        ('tp_src', int),
                        ('tp_dst', int)]

        self.fields = {}
        for name, typ in self.headers:
            setattr(self, name, self.iterate_and_fill(name, typ, params))
        self.create_list()

    def create_list(self): 
        """
        self.fields:
        {"field_name" -> self.field_name}
        """
        for name, typ in self.headers:
            self.fields[name] = (getattr(self, name), typ)

    @classmethod
    def from_row(cls, params):
        """
        Create a match from nesessary fields of row
        """
        match = FlowMatch({})
        for name, typ in match.headers:
            setattr(match, name, [match.convert_to_type(name, typ, params)])
        match.create_list()
        return match

    def convert_to_type(self, field, typ, params):
        if field in params.keys():
            try:
                return typ(params[field])
            except:
                pass

    def iterate_and_fill(self, field, typ, params):
        """
        Iterates params[field], converts to typ and returns as a list
        """
        result = []
        if field in params.keys():
            l = params[field]
            if not isinstance(l, collections.Iterable) or isinstance(l, basestring):
                l = [l]
            for item in l:
                if item is None:
                    result.append(None)
                else:
                    if typ is not IPAddr:
                        result.append(typ(item))
                    else:
                        result.append(parseCIDR(item, False))
        return result

    def iterate_and_str(self, field, l, line_sep='\n', val_sep='\n'):
        s = '%-10s' % (field) + line_sep
        prefix = '\t'
        for item in l:
            s += prefix + str(item) + val_sep
        return s

    def make_condition(self, table):
        """
        Make sql "WHERE" condition from match
        """
        result = ''
        first = True
        for field, typ in self.headers:
            list, type = self.fields[field]
            r = self.iterate_condition(table, field, list, type)
            if r is not None:
                if first:
                    result += '(' + r + ')' + '\n'
                    first = False
                else:
                    result += ' and ' + '(' + r + ')' + '\n'
        return result


    def iterate_condition(self, table, name, l, typ):
        """
        Make condition for single parameter (list of values)
        """
        if len(l) == 0:
            return None
        else:
            s = ''
            nullable = False
            first = True
            field = table + '.' + name
            if typ is IPAddr:
                s += ' '
                for item in l:
                    if item is None:
                        nullable = True
                        continue
                    ip, wc = item

                    if first:
                        first = False
                    else:
                        s += ' \nor '
                    if wc == 0:
                        s += ' ' + field + ' = ' + str(ip.toUnsigned())
                    else:
                        s += ' ' + field + ' between ' + str(ip.toUnsigned()) + \
                        ' and ' + str(ip.toUnsigned() + (1 << wc) - 1)
            else:
                s += ' ' + field + ' in ' + '('
                for item in l:
                    if item is None:
                        nullable = True
                        continue
                    if first:
                        first = False
                    else:
                        s += ', '
                    if typ is EthAddr:
                        s += str(item.toInt())
                    else:
                        s += str(item)
                s += ')'
                if first: s = ''
            if nullable:
                if not first:
                    s += ' \nor ' 
                s += field + ' is NULL '
            return s

    

    def __str__(self):
        result = ''
        for field, typ in self.headers:
            list, type = self.fields[field]
            result += self.iterate_and_str(field, list, line_sep='\n', val_sep='\n')
        return result

    def toShortString(self):
        result = ''
        for field, typ in self.headers:
            list, type = self.fields[field]
            result += self.iterate_and_str(field, list, line_sep='\t', val_sep='\n')
        return result



class HistoryMatch:
    """
    Class to store information about dpid/port/time of PacketIn event
    """
    def __init__(self, params):
        self.points = []
        if "point" in params.keys():
            l = params["point"]
            if not isinstance(l, collections.Iterable) or len(l) == 0 \
                or not isinstance(l[0], collections.Iterable):
                l = [l]
            for item in l:
                dpid, port = item
                self.points.append((int(dpid), int(port)))
        self.times = []
        if "time" in params.keys():
            l = params["time"]
            if not isinstance(l, collections.Iterable) or len(l) == 0 \
                    or not isinstance(l[0], collections.Iterable) \
                    or isinstance(l[0], basestring):
                l = [l]
            for item in l:
                start, finish = item
                self.times.append((time.strptime(start, "%Y-%m-%d %H:%M:%S"),
                    time.strptime(finish, "%Y-%m-%d %H:%M:%S")))

    def make_condition(self, table):
        """
        Make sql condition for the time and points
        """
        result = ' '
        first = True
        point_entry = '('
        for point in self.points:
            dpid, port = point
            s = ''
            if dpid != 0:
                s += table + '.' + 'dpid' + ' = ' + str(dpid)
                if port != 0:
                    s += ' and ' + table + '.' + 'in_port' + ' = ' + str(port)
            elif port != 0:
                s += table + '.' + 'in_port' + ' = ' + str(port)
            if s == '': continue
            if first:
                first = False
            else:
                point_entry += ' \nor '
            point_entry += '(' + s + ')'
        point_entry += ')'
        if first: point_entry = ''

        first = True
        time_entry = '('
        for t in self.times:
            start, finish = t
            s = ''
            s += table + '.' + 'time' + ' between \'' + \
                    time.strftime("%Y-%m-%d %H:%M:%S", start)
            s += '\' and \'' + time.strftime("%Y-%m-%d %H:%M:%S", finish) + '\''
            if first:
                first = False
            else:
                time_entry += ' \nor '
            time_entry += '(' + s + ')'
        time_entry += ')'
        if first: time_entry = ''
        
        if point_entry != '':
            result += ' ' + point_entry + '\n'
            if time_entry != '':
                result += ' and ' + time_entry + '\n'
        elif time_entry != '':
            result += ' ' + time_entry + '\n'

        return result

    def __str__(self):
        result = ''
        result += 'points\n'
        prefix = '    '
        for point in self.points:
            result += prefix + str(point[0]) + ':' + str(point[1]) + '\n'
        result += 'times\n'
        for t in self.times:
            result += prefix + time.strftime("%Y-%m-%d %H:%M:%S", t[0]) + \
                    prefix + time.strftime("%Y-%m-%d %H:%M:%S", t[1]) + '\n'
        return result
  
class DBChecker:
    """
    Class for creating queries to database
    """
    def __init__(self):
        self.db = Database()

    def execute(self, query, cursor = None, output = FULL):
        """
        Execute given query and output result
        """
        try:
            rows = self.db.execute_query(query, cursor)
            if rows is not None:
                if output == FULL:
                    print len(rows)
                    for row in rows:
                        s = ""
                        for item in row:
                            s = s + str(item) + "\t"
                        print s
                elif output == COUNT:
                    print len(rows)
                elif output == RETURN:
                    return rows
            else:
                print "Success"
        except Exception as e:
            print e

    def finish(self):
        self.db.disconnect()

    def choose_by_params(self, **params):
        """
        process user input and make query to database
        """
        match = FlowMatch(params)
        time = HistoryMatch(params)
        query = self.make_query(match, time)
        self.execute(query)

    def trace_path(self, **params):
        """
        Check whether subflow goes through check_point
        1. Create match from params
        2. Ask DB for matching PacketIns
        3. Draw full graph and each subflow graph
        4. Check whether subgraph is checkable with our algorithm
        5. If checkable, trace path of subflow to check_point
        6. Output dropped packets to log
        """
        #Flow header fields
        match = FlowMatch(params)
        #time and point(?)
        time = HistoryMatch(params)
        query = self.make_query(match, time)
        rows = self.execute(query, cursor = Database.DICTIONARY, output = RETURN)
        
        '''
        for row in rows:
            print row["src_dpid"], ":", row["src_port"], \
                " ---> ", row["dpid"], ":", row["in_port"]
        '''
        full_graph = self.graph_from_routes(rows)

        #separate packets by headers
        flow_routes = {}
        for row in rows:
            if row["ID"] in flow_routes:
                flow_routes[row["ID"]].append(row)
            else:
                flow_routes[row["ID"]] = [row]

        if 'check_point' in params:
            check_point = int(params['check_point'])
        else:
            check_point = None

        if 'print_flow' in params:
            print_flow = bool(params['print_flow'])
        else:
            print_flow = True


        if 'draw' in params:
            draw = bool(params['draw'])
        else:
            draw = False

        #analyze each flow separately
        for ID, routes in flow_routes.iteritems():
            self.check_flow(routes, full_graph, ID, check_point, draw=draw, print_flow=print_flow)

        self.check_flow(rows, draw=True)
        print '%d rows processed' % (len(rows))
        log.flush()

    def graph_from_routes(self, routes, single_inout=False):
        #Count packets on each link
        paths = {}
        for row in routes:
            t = (row["src_dpid"], row["src_port"], \
                row["dpid"], row["in_port"])
            if t in paths:
                paths[t] += 1
            else:
                paths[t] = 1
            
            '''
            print ID
            for link, num in paths.iteritems():
                print link[0], ":", link[1], " ---> ", link[2], ":", link[3], "   ", num
            '''

        g = self.create_graph(paths, single_inout)
        return g


    def check_flow(self, routes, full_graph = None, ID=None, check_point=None, \
            draw=True, print_flow=True):
        g = self.graph_from_routes(routes)
        if draw:
            if ID is not None:
                self.draw_graph(g, str(ID))
            else:
                self.draw_graph(g, '')

        
        functions = [(self.is_tree_checkable, self.naive_tree_check), \
                     (self.is_not_so_naive_tree_checkable, self.not_so_naive_tree_check)]

        is_checkable, check = functions[1]
        #try to check tree
        #TODO: working algorithm
        if check_point is not None:
            log.info('\n\n' + str(ID))
            if len(routes) > 0 and print_flow:
                log.info('\n' + FlowMatch.from_row(routes[0]).toShortString())
            #Check whether our graph is checkable with our algorithm
            g_check = self.graph_from_routes(routes, single_inout=True)
            if is_checkable(g_check):
                check(g, full_graph, check_point)
            else:
                log.info("Graph is not checkable with current algorithm")

    def is_not_so_naive_tree_checkable(self, graph):
        """
        We don't want two routes using at least two different points
        of an inner cycle unless cycle is hanging (only one entrance).
        """
        cycles = nx.simple_cycles(graph)
        paths = []
        inner_cycles = []
        points_in_paths = set()
        points_in_inner = set()

        for cycle in cycles:
            if 'Input' in cycle:
                paths.append(cycle)
                points_in_paths |= set(cycle)
            else:
                points = 0
                for point in cycle[:-1]:
                    for pred in graph.predecessors(point):
                        if pred not in cycle:
                            points += 1
                            break
                if points > 1:    
                    inner_cycles.append(cycle)
                    points_in_inner |= set(cycle)
                else:
                    log.debug('hanging loop %s' % (str(cycle)))

        log.debug ('paths: %s\ninner: %s' % (points_in_paths, points_in_inner))

        if len(points_in_paths & points_in_inner) > 1:
            return False
        else:
            return True

    def is_tree_checkable(self, graph):
        """
        We don't want two cycles len > 2 using
        the same edge in different directions
        O(n^2 * m^2)
        """
        cycles = nx.simple_cycles(graph)
        log.debug("cycles:")
        log.debug("%s" % (str(cycles)))
        
        for cycle1 in range(len(cycles)):
            if len(cycles[cycle1]) <= 3:
                continue
            for cycle2 in range(cycle1 + 1, len(cycles)):
                if len(cycles[cycle2]) <= 3:
                    continue
                for i1 in range(1, len(cycles[cycle1])):
                    for i2 in range(1, len(cycles[cycle2])):
                        if cycles[cycle1][i1] == cycles[cycle2][i2-1] \
                            and cycles[cycle1][i1-1] == cycles[cycle2][i2] \
                            and cycles[cycle1][i1] != 'Input' \
                            and cycles[cycle1][i1-1] != 'Input':
                            log.debug("False on %d and %d" % (cycle1, cycle2))
                            return False
        
        '''
        pairs = []
        reversed_pairs = []
        for cycle_id in range(len(cycles)):
            pairs.append([])
            reversed_pairs.append([])
            if len(cycles[cycle_id]) <= 3:
                continue
            for i in range(1, len(cycles[cycle_id])):
                if cycles[cycle_id][i] != 'Input' \
                        and cycles[cycle_id][i-1] != 'Input':
                    pairs[cycle_id].append((cycles[cycle_id][i-1], \
                            cycles[cycle_id][i]))
                    reversed_pairs[cycle_id].append((cycles[cycle_id][i], \
                            cycles[cycle_id][i-1]))
            pairs[cycle_id].sort()
            reversed_pairs[cycle_id].sort()

        for cycle1 in range(len(cycles)):
            for cycle2 in range(cycle1 + 1, len(cycles)):
                i1 = i2 = 0
                while i1 < len(pairs[cycle1]) and i2 < len(reversed_pairs[cycle2]):
                    if pairs[cycle1][i1] == reversed_pairs[cycle2][i2]:
                        log.debug("False on %d and %d" % (cycle1, cycle2))
                        return False
                    elif pairs[cycle1][i1] < reversed_pairs[cycle2][i2]:
                        i1 += 1
                    else:
                        i2 += 1

        '''
        '''
        pairs = []
        reversed_pairs = []
        for cycle_id in range(len(cycles)):
            if len(cycles[cycle_id]) <= 3:
                continue
            for i in range(1, len(cycles[cycle_id])):
                if cycles[cycle_id][i] != 'Input' \
                        and cycles[cycle_id][i-1] != 'Input':
                    pairs.append((cycles[cycle_id][i-1], \
                            cycles[cycle_id][i]))
                    reversed_pairs.append((cycles[cycle_id][i], \
                            cycles[cycle_id][i-1]))
        pairs.sort()
        reversed_pairs.sort()


        i1 = i2 = 0
        while i1 < len(pairs) and i2 < len(reversed_pairs):
            if pairs[i1] == reversed_pairs[i2]:
                log.debug("False")
                return False
            elif pairs[i1] < reversed_pairs[i2]:
                i1 += 1
            else:
                i2 += 1
        '''
        return True

    def not_so_naive_tree_check(self, g, full_graph, check_point):
        """
        Check flow tree using our algorithm
        """
        routes = {}
        list_routes = []
        error = False
        #Create a path from each start node to check_point
        for start in [node_id for node_id in g.nodes() \
            if isinstance(node_id, basestring) and 'Input' in node_id]:
            #routes[start] = []
            count = 0
            paths = []
            try:
                #Find a way to check in our subgraph
                paths = nx.all_simple_paths(g, source=start, target=check_point)
            except Exception as e:
                log.debug(str(e))
                try:
                    #Try to find a path in full graph
                    #TODO: maybe we need only one route
                    #not to get an error below
                    paths = []
                    for succ in g.successors(start):
                        paths.extend(nx.all_simple_paths(full_graph, \
                                source=succ, target=check_point))
                    #We are the start point on each route
                    for p in paths:
                        p.insert(0, start)
                except Exception as e:
                        log.debug(str(e))
                
            #Create a list of all routes
            first_hops = []
            hop_in = 0
            for path in paths:
                log.info('path : %s' % (str(path)))
                if path[1] not in first_hops:
                    first_hops.append(path[1])
                    hop_in += g[start][path[1]]['weight']
                #path.reverse()
                #routes[start].append(path)
                list_routes.append(path)
                count += 1
            log.info('Packets from this source: %d' % (hop_in))

            #We dont want different paths or no paths
            if count == 0:
                error = True
                log.info('No path from %s to %s' % (start, str(check_point)))
            elif count > 1:
                #error = True
                log.info('Found %d routes from %s to %s' % \
                        (count, start, str(check_point)))
        if error:
            return False

    
        #Analyze each route from check to start
        return self.check_simple_paths(g, list_routes)

    def check_simple_paths(self, g, list_routes):
        class Vertex:
            def __init__(self):
                self.enter = {}
                self.exit = {}

            def __str__(self):
                s = 'enter: %s\nexit: %s\n' % (str(self.enter), str(self.exit))
                return s

        vertices = {}
        for route in list_routes:
            for i in range(1, len(route)-1):
                #TODO: Probably going on edges instead of vertices
                #      can be twice more efficient
                if route[i] in vertices:
                    vertex = vertices[route[i]]
                else:
                    vertex = Vertex()

                if route[i-1] not in vertex.enter:
                    vertex.enter[route[i-1]] = g[route[i-1]][route[i]]['weight']
                if route[i+1] not in vertex.exit:
                    vertex.exit[route[i+1]] = g[route[i]][route[i+1]]['weight']

                vertices[route[i]] = vertex

        for name, vertex in vertices.iteritems():
            log.debug('name: %s inside: \n %s' % (str(name), str(vertex)))
            enter = exit = 0
            for name, val in vertex.enter.iteritems():
                enter += val
            for name, val in vertex.exit.iteritems():
                exit += val

            if enter > exit:
                log.info('Lost %d packets in %s' % (enter - exit, str(name)))
            elif enter < exit:
                log.info('Error in %s: in = %d, out = %d' \
                        % (str(name), enter, exit))

    def naive_tree_check(self, g, full_graph, check_point):
        """
        Check flow tree using our algorithm
        """
        routes = {}
        list_routes = []
        error = False
        #Create a path from each start node to check_point
        for start in [node_id for node_id in g.nodes() \
            if isinstance(node_id, basestring) and 'Input' in node_id]:
            #routes[start] = []
            count = 0
            paths = []
            try:
                #Find a way to check in our subgraph
                paths = nx.all_simple_paths(g, source=start, target=check_point)
            except Exception as e:
                log.debug(str(e))
                try:
                    #Try to find a path in full graph
                    #TODO: maybe we need only one route
                    #not to get an error below
                    paths = []
                    for succ in g.successors(start):
                        paths.extend(nx.all_simple_paths(full_graph, \
                                source=succ, target=check_point))
                    #We are the start point on each route
                    for p in paths:
                        p.insert(0, start)
                except Exception as e:
                        log.debug(str(e))
                
            #Create a list of all routes
            first_hops = []
            hop_in = 0
            for path in paths:
                log.info('path : %s' % (str(path)))
                if path[1] not in first_hops:
                    first_hops.append(path[1])
                    hop_in += g[start][path[1]]['weight']
                path.reverse()
                #routes[start].append(path)
                list_routes.append(path)
                count += 1
            log.info('Packets from this source: %d' % (hop_in))

            #We dont want different paths or no paths
            if count == 0:
                error = True
                log.info('No path from %s to %s' % (start, str(check_point)))
            elif count > 1:
                #error = True
                log.info('Found %d routes from %s to %s' % \
                        (count, start, str(check_point)))
        if error:
            return False

    
        #Analyze each route from check to start
        return self.traverse(g, list_routes)


    def traverse(self, g, list_routes):
        """
        Analyze single step on every route from check to start.
        Then call recursively.
        """
        success = True
        next_hops = {}
        if len(list_routes) == 0 or len(list_routes[0]) == 0:
            return success
        current = list_routes[0][0]
        for route in list_routes:
            if len(route) <= 2:
                log.debug("Finish on %s" % (str(route[0])))
            else:
                if route[1] not in next_hops:
                    next_hops[route[1]] = []
                if route[2] not in next_hops[route[1]]:
                    next_hops[route[1]].append(route[2])

        log.debug("Next hops: %s" % (str(next_hops)))
        for next_hop in next_hops.keys():
            in_next = 0
            traverse = True
            next_routes = []
            for route in [route for route in list_routes \
                    if len(route) > 2 and route[1] == next_hop]:
                next_routes.append(route[1:]) 

            for next_next in next_hops[next_hop]:
                try:
                    tmp = g[next_next][next_hop]['weight']
                    log.debug("%s -> %s %d" % (str(next_next), str(next_hop), tmp))
                    in_next += tmp
                except:
                    log.debug("%s -> %s No route" % (str(next_next), str(next_hop)))
            try:
                out_next = g[next_hop][current]['weight']
                log.debug("%s -> %s %d" % (str(next_hop), str(current), out_next))
            except:
                log.debug('%s -> %s No route' % (str(next_hop), str(current)))
                out_next = 0
            
            if out_next < in_next:
                log.info('Lost %d packets in %s' % (in_next-out_next, str(next_hop)))
                success = False
            elif out_next > in_next:
                log.info('Error in %s : in - %d, out - %d ' % \
                        (str(next_hop), in_next, out_next))
                success = False
            if traverse:
                log.debug('traverse with len %d' % (len(next_routes)))
                success &= self.traverse(g, next_routes)

        return success
    
    def create_graph(self, paths, single_inout=False):

        g = nx.DiGraph()

        inputs = 0
        for link, num in paths.iteritems():
            l = link[0]
            if link[0] is not None:
                if link[0] not in g.nodes():
                    g.add_node(link[0])
            else:
                if not single_inout:
                    l = "Input" + str(inputs)
                    g.add_node(l, fillcolor='green', style='filled')
                    inputs += 1
                elif 'Input' not in g.nodes():
                        l = "Input"
                        g.add_node(l, fillcolor='green', style='filled')
            if link[2] not in g.nodes():
                g.add_node(link[2])

            g.add_edge(l, link[2], weight=num, label=num, src_port=link[1], \
                    taillabel=link[1], in_port=link[3], headlabel=link[3], len=2)

        if single_inout:
            for node_id in g.nodes():
                if node_id == 'Input':
                    continue
                inp = 0
                out = 0
                for target in g.successors(node_id):
                    out += g[node_id][target]['weight']
                for target in g.predecessors(node_id):
                    inp += g[target][node_id]['weight']
                if inp > out:
                    g.node[node_id]['fillcolor'] = 'blue'
                    g.node[node_id]['style'] = 'filled'
                    g.add_edge(node_id, 'Input', weight=inp-out, label=inp-out, len=2)

        return g
            
    def draw_graph(self, g, ID=''):
        if ID is None:
            ID = ''
        pos = nx.spring_layout(g)
        center_labels=dict([((u,v,),d['label'])\
            for u,v,d in g.edges(data=True)])
        start_labels=dict([((u,v,),d['taillabel'])\
            for u,v,d in g.edges(data=True)])
        end_labels=dict([((u,v,),d['headlabel'])\
            for u,v,d in g.edges(data=True)])


        for node_id in g.nodes():
            inp = 0
            out = 0
            for target in g.successors(node_id):
                out += g[node_id][target]['label']
            for target in g.predecessors(node_id):
                inp += g[target][node_id]['label']
            if g.successors(node_id) == [] or inp > out:
                g.node[node_id]['fillcolor'] = 'blue'
                g.node[node_id]['style'] = 'filled'

        nx.draw(g, pos)
        nx.draw_networkx_edges(g, pos, arrows=True)
        nx.draw_networkx_edge_labels(g,pos,edge_labels=center_labels)
        nx.draw_networkx_edge_labels(g,pos,edge_labels=end_labels, label_pos=0.1)
        nx.draw_networkx_edge_labels(g, pos, label_pos=0.9, edge_labels=start_labels)
        nx.write_dot(g, 'graph' + ID + '.dot')


    def make_query(self, match, time = None):
        """
        Construct a sql-query from given flow properties
        """
        query = ''
        query += 'Select * from FlowHistory left join FlowMatch '
        query += 'on (FlowHistory.ID = FlowMatch.ID)'
        cond = match.make_condition('FlowMatch')
        t = ''
        if time is not None:
            t = time.make_condition('FlowHistory')
        if cond != '' and not cond.isspace():
            query += ' where\n' + cond
            if t != '' and not t.isspace():
                query += 'and\n' + t
        elif t != '' and not t.isspace():
            query += ' where\n' + t
        print query
        return query


import MySQLdb as mdb
import sys
import pox.lib.packet as pkt
import pox.openflow.libopenflow_01 as of
import time

class Database:

  SIMPLE = 0
  DICTIONARY = 1

  def __init__(self):
    self.connect()
    self.create_tables()
    

  def connect(self):
    try:
      self.con = mdb.connect('localhost', 'user', 
        '1234', 'Traffic')
    except:
      pass
        
  def create_tables(self):
    if self.con:
      cur = self.con.cursor()
      cur.execute("CREATE TABLE IF NOT EXISTS FlowMatch( \
           ID INT UNSIGNED PRIMARY KEY AUTO_INCREMENT, \
           dl_src BIGINT UNSIGNED, \
           dl_dst BIGINT UNSIGNED, \
           dl_vlan SMALLINT UNSIGNED, \
           dl_vlan_pcp TINYINT UNSIGNED, \
           dl_type SMALLINT UNSIGNED, \
           nw_tos TINYINT UNSIGNED, \
           nw_proto TINYINT UNSIGNED, \
           nw_src INT UNSIGNED, \
           nw_dst INT UNSIGNED, \
           tp_src SMALLINT UNSIGNED, \
           tp_dst SMALLINT UNSIGNED \
           )")
      cur.execute("CREATE TABLE IF NOT EXISTS FlowHistory( \
           ID INT UNSIGNED, \
           time DATETIME, \
           dpid BIGINT, \
           in_port SMALLINT UNSIGNED, \
           src_dpid BIGINT, \
           src_port SMALLINT UNSIGNED \
           )")
      
  def disconnect(self):
    if self.con: 
      self.con.close()
      
  def clear(self):
    if self.con:
      cur = self.con.cursor()
      cur.execute("DELETE FROM FlowMatch")
      cur.execute("DELETE FROM FlowHistory")
      
  def add_flow(self, packet):
    if self.con:
      match = of.ofp_match.from_packet(packet)
      cur = self.con.cursor()
      query = "INSERT INTO FlowMatch(dl_src, dl_dst, dl_vlan, dl_vlan_pcp, \
            dl_type, nw_tos, nw_proto, nw_src, nw_dst, tp_src, tp_dst) \
            VALUES (%s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s)" % \
            (match.dl_src.toInt(), match.dl_dst.toInt(), match.dl_vlan, \
	    match.dl_vlan_pcp, match.dl_type, match.nw_tos, match.nw_proto, \
	    match.nw_src.toUnsigned() if match.nw_src else match.nw_src, \
            match.nw_dst.toUnsigned() if match.nw_dst else match.nw_dst, \
            match.tp_src, match.tp_dst)
      query = query.replace("None", "NULL")
      #print(query)
      cur.execute(query)
      
  def add_history(self, packet, dpid, in_port, src_dpid = None, src_port = None, t = None):
    if self.con:
      
      match = of.ofp_match.from_packet(packet)
      cur = self.con.cursor()
      query = "SELECT ID FROM FlowMatch WHERE dl_src = %s AND dl_dst = %s AND \
       dl_vlan = %s AND dl_vlan_pcp = %s AND dl_type = %s AND nw_tos = %s AND nw_proto = %s AND\
       nw_src = %s AND nw_dst = %s AND tp_src = %s AND tp_dst = %s" % \
       (match.dl_src.toInt(), match.dl_dst.toInt(), match.dl_vlan, match.dl_vlan_pcp, match.dl_type, \
       match.nw_tos, match.nw_proto, match.nw_src.toUnsigned() if match.nw_src else match.nw_src, \
       match.nw_dst.toUnsigned() if match.nw_dst else match.nw_dst, match.tp_src, \
       match.tp_dst)
      query = query.replace("= None", " IS NULL")
      #print(query)
      cur.execute(query)
      id = cur.fetchone()
      if id is None:
        self.add_flow(packet)
        cur.execute(query)
        id = cur.fetchone()
        if id is None:
          print("DB ERROR: Unable to add flow")
      if t is None:
        t = time.gmtime()
      query = "INSERT INTO FlowHistory(ID, time, dpid, in_port, src_dpid, \
              src_port) VALUES (%d, \"%s\", %d, %d, %s, %s )" \
              % (int(id[0]), time.strftime("%Y-%m-%d %H:%M:%S", t), \
                dpid, in_port, str(src_dpid), str(src_port))
      query = query.replace("None", "NULL")
      #print(query)
      cur.execute(query)

  def execute_query(self, query, cursor = None):
    if self.con:
      cur = None
      if cursor is None or cursor == self.SIMPLE:
        cur = self.con.cursor()
      elif cursor == self.DICTIONARY:
        cur = self.con.cursor(mdb.cursors.DictCursor)
      if cur is None:
        return None
      cur.execute(query)
      rows = cur.fetchall()
      return rows
      
import socket
import os
import sys
import exceptions
from errno import EAGAIN, ECONNRESET

from pox.openflow.of_01 import deferredSender, classes
import pox.openflow.libopenflow_01 as of
from pox.lib.revent import EventMixin, Event
from naive_logger import log

class MessageIn( Event ):
  def __init__(self, connection, ofp):
    Event.__init__(self)
    self.connection = connection
    self.ofp = ofp

class StatsIn( Event ):
  def __init__(self, connection, parts):
    Event.__init__(self)
    self.connection = connection
    self.ofp = parts

class BaseConnection (EventMixin):
  """
  A Connection object represents a single TCP session with an
  openflow-enabled switch.
  If the switch reconnects, a new connection object is instantiated.
  """
  """
  _eventMixin_events = set([
    ConnectionUp,
    ConnectionDown,
    PortStatus,
    FlowRemoved,
    PacketIn,
    ErrorIn,
    BarrierIn,
    RawStatsReply,
    SwitchDescReceived,
    FlowStatsReceived,
    AggregateFlowStatsReceived,
    TableStatsReceived,
    PortStatsReceived,
    QueueStatsReceived,
    FlowRemoved,
  ])
  """
  _eventMixin_events = set([
    MessageIn                          
  ])
  # Globally unique identifier for the Connection instance
  #ID = 0

  def msg (self, m):
    #print str(self), m
    log.debug(str(self) + " " + str(m))
  def err (self, m):
    #print str(self), m
    log.error(str(self) + " " + str(m))
  def info (self, m):
    #print str(self), m
    log.info(str(self) + " " + str(m))

  def __init__ (self, sock = None, ID = None):
    self._previous_stats = []

    #self.ofnexus = _dummyOFNexus
    self.sock = sock
    self.buf = ''
    self.ID = ID
    # TODO: dpid and features don't belong here; they should be eventually
    # be in topology.switch
    #self.dpid = None
    #self.features = None
    self.disconnected = False
    self.connect_time = None

    #self.send(of.ofp_hello())

    #TODO: set a time that makes sure we actually establish a connection by
    #      some timeout

  def fileno (self):
    return self.sock.fileno()

  def close (self):
    if not self.disconnected:
      self.info("closing connection")
    else:
      #self.msg("closing connection")
      pass
    try:
      self.sock.shutdown(socket.SHUT_RDWR)
    except:
      pass
    try:
      self.sock.close()
    except:
      pass

  def disconnect (self):
    """
    disconnect this Connection (usually not invoked manually).
    """
    if self.disconnected:
      self.err("already disconnected!")
    self.msg("disconnecting")
    self.disconnected = True
    try:
      self.sock.shutdown(socket.SHUT_RDWR)
    except:
      pass
    if self.dpid != None:
      self.raiseEventNoErrors(ConnectionDown, self)

  def send (self, data):
    """
    Send raw data to the switch.

    Generally, data is a bytes object.  If not, we check if it has a pack()
    method and call it (hoping the result will be a bytes object).  This
    way, you can just pass one of the OpenFlow objects from the OpenFlow
    library to it and get the expected result, for example.
    """
    if self.disconnected: return
    if type(data) is not bytes:
      if hasattr(data, 'pack'):
        data = data.pack()

    if deferredSender.sending:
      log.debug("deferred sender is sending!")
      deferredSender.send(self, data)
      return
    try:
      l = self.sock.send(data)
      if l != len(data):
        self.msg("Didn't send complete buffer.")
        data = data[l:]
        deferredSender.send(self, data)
    except socket.error as (errno, strerror):
      if errno == EAGAIN:
        self.msg("Out of send buffer space.  " +
                 "Consider increasing SO_SNDBUF.")
        deferredSender.send(self, data)
      else:
        self.msg("Socket error: " + strerror)
        self.disconnect()

  def read (self):
    """
    Read data from this connection.  Generally this is just called by the
    main OpenFlow loop below.

    Note: This function will block if data is not available.
    """
    #log.debug("Reading from " + str(self))
    d = self.sock.recv(2048)
    if len(d) == 0:
      return False
    self.buf += d
    l = len(self.buf)
    while l > 4:
      if ord(self.buf[0]) != of.OFP_VERSION:
        log.debug("Bad OpenFlow version (" + str(ord(self.buf[0])) +
                    ") on connection " + str(self))
        return False
      # OpenFlow parsing occurs here:
      ofp_type = ord(self.buf[1])
      packet_length = ord(self.buf[2]) << 8 | ord(self.buf[3])
      if packet_length > l: break
      msg = classes[ofp_type]()
      #log.debug("RECEIVED " + msg.__class__.__name__)
      # msg.unpack implicitly only examines its own bytes, and not trailing
      # bytes 
      msg.unpack(self.buf)
      self.buf = self.buf[packet_length:]
      l = len(self.buf)
      #proxy.process_message(self, msg)
      self.raiseEventNoErrors(MessageIn, self, msg)
    return True

  def _incoming_stats_reply (self, ofp):
    # This assumes that you don't receive multiple stats replies
    # to different requests out of order/interspersed.
    more = (ofp.flags & 1) != 0
    if more:
      if ofp.type not in [of.OFPST_FLOW, of.OFPST_TABLE,
                                of.OFPST_PORT, of.OFPST_QUEUE]:
        log.error("Don't know how to aggregate stats message of type " +
                  str(ofp.type))
        self._previous_stats = []
        return
      
    if len(self._previous_stats) != 0:
      if ((ofp.xid == self._previous_stats[0].xid) and
          (ofp.type == self._previous_stats[0].type)):
        self._previous_stats.append(ofp)
      else:
        log.error("Was expecting continued stats of type %i with xid %i, " +
                  "but got type %i with xid %i" %
                  (self._previous_stats_reply.xid,
                    self._previous_stats_reply.type,
                    ofp.xid, ofp.type))
        self._previous_stats = [ofp]
    else:
      self._previous_stats = [ofp]

    if not more:
      #handler = statsHandlerMap.get(self._previous_stats[0].type, None)
      s = self._previous_stats
      self._previous_stats = []
      #if handler is None:
      #  log.warn("No handler for stats of type " +
        #         str(self._previous_stats[0].type))
      #  return
      #handler(self, s)

      #TODO: Special messages for stats
      self.raiseEventsNoErrors(StatsIn, self, s)

  def __str__ (self):
    return "[Con-base " + str(self.ID) + "]"

class SwitchConnection (BaseConnection):
  def __init__(self, sock = None, ID = None):
    BaseConnection.__init__(self, sock, ID)
    
  def __str__ (self):
    return "[Con-switch " + str(self.ID) + "]"
  
class ControllerConnection(BaseConnection):
  def __init__(self, port = 6634, address = "0.0.0.0", ID = None):
    sock = socket.create_connection((address, port))
    BaseConnection.__init__(self, sock, ID)
    
  def __str__ (self):
    return "[Con-controller " + str(self.ID) + "]"
from mininet.topo import Topo
from collections import namedtuple

"""
              

          s4(loopback)----s3      badH (10.0.1.1)
                         /  \    /
                        /    \  /
(10.0.0.2)innerH------s2----- s1-------outerH(10.0.0.1)

"""
class Blocks(Topo):
    def __init__(self):
        Topo.__init__(self)        
        
        switches = ["s1", "s2", "s3", "s4"]
        host_opts = namedtuple('host_opts', 'ip mac')
        hosts = {"innerH":host_opts("10.0.0.2", "00:00:00:00:00:02"), 
                "outerH":host_opts("10.0.0.1", "00:00:00:00:00:01"), 
                "badH":host_opts("10.0.1.1", "00:00:00:00:00:03")}
        nodes = {}



        for s in switches:
            nodes[s] = self.addSwitch(s)
            
        for h in hosts.keys():
            nodes[h] = self.addHost(h, **hosts[h]._asdict())        
            
        self.addLink(nodes["outerH"], nodes["s1"])
        self.addLink(nodes["s1"], nodes["s2"])
        self.addLink(nodes["s1"], nodes["s3"])
        self.addLink(nodes["s3"], nodes["s4"])
        self.addLink(nodes["s3"], nodes["s2"])
        self.addLink(nodes["s2"], nodes["innerH"])
        self.addLink(nodes["badH"], nodes["s1"])
        
topos = { 'blocks': ( lambda: Blocks() ) }
        
from mininet.topo import Topo
from collections import namedtuple

"""
              

          s4(loopback)----s3      badH (10.0.1.1)
                         /  \    /
                        /    \  /
(10.0.0.2)innerH------s2----- s1-------outerH(10.0.0.1)

"""
class Blocks(Topo):
    def __init__(self):
        Topo.__init__(self)        
        
        switches = ["s1", "s2", "s3", "s4"]
        host_opts = namedtuple('host_opts', 'ip mac')
        hosts = {"innerH":host_opts("10.0.0.2", "00:00:00:00:00:02"), 
                "outerH":host_opts("10.0.0.1", "00:00:00:00:00:01"), 
                "badH":host_opts("10.0.1.1", "00:00:00:00:00:03")}
        nodes = {}



        for s in range(1, 9):
            nodes[s] = self.addSwitch('s'+str(s))
            
        #for h in hosts.keys():
        #    nodes[h] = self.addHost(h, **hosts[h]._asdict())        
        '''    
        self.addLink(nodes["outerH"], nodes["s1"])
        self.addLink(nodes["s1"], nodes["s2"])
        self.addLink(nodes["s1"], nodes["s3"])
        self.addLink(nodes["s3"], nodes["s4"])
        self.addLink(nodes["s3"], nodes["s2"])
        self.addLink(nodes["s2"], nodes["innerH"])
        self.addLink(nodes["badH"], nodes["s1"])
        '''
        self.addHost('h1')
        #self.addHost('h2')
        #self.addHost('h7')
        self.addHost('h8')
        #self.addHost('h6')
        #self.addHost('h3')
        
        self.addLink('s1', 's2')
        self.addLink('s1', 's3')
        self.addLink('s1', 's4')
        self.addLink('s2', 's5')
        self.addLink('s2', 's6')
        self.addLink('s2', 's3')
        self.addLink('s3', 's4')
        self.addLink('s3', 's5')
        self.addLink('s3', 's6')
        self.addLink('s3', 's7')
        self.addLink('s4', 's6')
        self.addLink('s4', 's7')
        self.addLink('s5', 's6')
        self.addLink('s5', 's8')
        self.addLink('s6', 's7')
        self.addLink('s6', 's8')
        self.addLink('s7', 's8')

        self.addLink('s1', 'h1')
        #self.addLink('s2', 'h2')
        #self.addLink('s7', 'h7')
        self.addLink('s8', 'h8')
        #self.addLink('s6', 'h6')
        #self.addLink('s3', 'h3')


topos = { 'blocks': ( lambda: Blocks() ) }
        
